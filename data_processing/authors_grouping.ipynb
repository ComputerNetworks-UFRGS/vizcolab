{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, glob, unicodedata, multiprocessing\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('⚙️ Importing authors...')\n",
    "\n",
    "path = os.path.join('datasets/autores/', \"autores-*.csv\")\n",
    "data_files = glob.glob(path) \n",
    "print(*data_files, sep = \"\\n\")\n",
    "\n",
    "df = pd.concat((pd.read_csv(f, encoding='iso8859_1', delimiter=\";\") for f in data_files))\n",
    "\n",
    "# Only selects author 5% sample of the dataset\n",
    "# df = df.sample(frac=0.05, random_state=1)\n",
    "\n",
    "print(\"   {} authors in the dataset\".format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Type Optimization\n",
    "# https://medium.com/bigdatarepublic/advanced-pandas-optimize-speed-and-memory-a654b53be6c2\n",
    "\n",
    "def optimize_floats(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    floats = df.select_dtypes(include=['float64']).columns.tolist()\n",
    "    df[floats] = df[floats].astype('int64')\n",
    "    return df\n",
    "\n",
    "def optimize_ints(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ints = df.select_dtypes(include=['int64']).columns.tolist()\n",
    "    df[ints] = df[ints].apply(pd.to_numeric, downcast='integer')\n",
    "    return df\n",
    "\n",
    "def optimize_objects(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in df.select_dtypes(include=['object']):\n",
    "        if not (type(df[col][0])==list):\n",
    "            num_unique_values = len(df[col].unique())\n",
    "            num_total_values = len(df[col])\n",
    "            if float(num_unique_values) / num_total_values < 0.5:\n",
    "                df[col] = df[col].astype('category')\n",
    "    return df\n",
    "\n",
    "def optimize(df: pd.DataFrame):\n",
    "    return optimize_floats(optimize_ints(optimize_objects(df)))\n",
    "\n",
    "print('⚙️ Optimizing columns data types...')\n",
    "\n",
    "optimize(df)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter fields of interest\n",
    "df = df.filter([\n",
    "  'AN_BASE',\n",
    "  'NM_AUTOR',\n",
    "  'NM_ABNT_AUTOR',\n",
    "  'TP_AUTOR',\n",
    "  'NM_TP_CATEGORIA_DOCENTE',\n",
    "  'NM_NIVEL_DISCENTE',\n",
    "  'CD_PROGRAMA_IES',\n",
    "  'NM_PROGRAMA_IES',\n",
    "  'NM_AREA_CONHECIMENTO',\n",
    "  'SG_ENTIDADE_ENSINO',\n",
    "  'ID_PESSOA_DISCENTE',\n",
    "  'ID_PESSOA_DOCENTE',\n",
    "  'ID_PESSOA_PART_EXTERNO',\n",
    "  'ID_PESSOA_POS_DOC',\n",
    "  'ID_PESSOA_EGRESSO',\n",
    "  'ID_ADD_PRODUCAO_INTELECTUAL',\n",
    "])\n",
    "\n",
    "# Unify IDs\n",
    "def unify_ids(cols):\n",
    "    return {\n",
    "      'DOCENTE': cols['ID_PESSOA_DOCENTE'],\n",
    "      'EGRESSO': cols['ID_PESSOA_EGRESSO'],\n",
    "      'PÓS-DOC': cols['ID_PESSOA_POS_DOC'],\n",
    "      'DISCENTE': cols['ID_PESSOA_DISCENTE'],\n",
    "      'PARTICIPANTE EXTERNO': cols['ID_PESSOA_PART_EXTERNO'],\n",
    "      '-': None,\n",
    "    }[cols['TP_AUTOR']]\n",
    "\n",
    "ids = [\n",
    "  'ID_PESSOA_DISCENTE',\n",
    "  'ID_PESSOA_DOCENTE',\n",
    "  'ID_PESSOA_PART_EXTERNO',\n",
    "  'ID_PESSOA_POS_DOC',\n",
    "  'ID_PESSOA_EGRESSO',\n",
    "]\n",
    "\n",
    "print('⚙️ Unifying author IDs...')\n",
    "\n",
    "df['ID'] = df[['TP_AUTOR', *ids]].apply(unify_ids, axis=1)\n",
    "df = df.drop(columns=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/517923/what-is-the-best-way-to-remove-accents-normalize-in-a-python-unicode-string\n",
    "def strip_accents(text):\n",
    "  text = unicodedata.normalize('NFD', text)\n",
    "  text = text.encode('ascii', 'ignore')\n",
    "  text = text.decode(\"utf-8\")\n",
    "  return str(text)\n",
    "\n",
    "def normalize_name(name):\n",
    "  if ',' not in name:\n",
    "    norm = name\n",
    "  else:\n",
    "    it = name.split(', ')\n",
    "    it.reverse()\n",
    "    norm = ' '.join(it)\n",
    "\n",
    "  # remove accents\n",
    "  norm = strip_accents(norm)\n",
    "\n",
    "  # remove invalid chars\n",
    "  norm = re.sub('[_-]', ' ', norm)\n",
    "  norm = re.sub('[0-9?&#;()]', '', norm)\n",
    "\n",
    "  # remove leading and trailing spaces\n",
    "  norm = norm.strip()\n",
    "\n",
    "  return norm\n",
    "\n",
    "print('⚙️ Normalizing df names...')\n",
    "df['NM_AUTOR'] = df['NM_AUTOR'].apply(normalize_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def firstAndLastName(name):\n",
    "  it = name.split(' ')\n",
    "  return ' '.join([it[0], it[-1]])\n",
    "\n",
    "print('⚙️ Creating helper columns...')\n",
    "df['FIRST_LAST_NAME'] = df['NM_AUTOR'].apply(firstAndLastName)\n",
    "df['FULL_NAME'] = df['NM_AUTOR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper Functions\n",
    "def agg_to_dict(items):\n",
    "  return items.astype(str).value_counts().to_dict()\n",
    "\n",
    "def most_frequent(items):\n",
    "  values = items.dropna()\n",
    "  if len(values) == 0: return None\n",
    "  occurence_count = Counter(values)\n",
    "  return occurence_count.most_common(1)[0][0]\n",
    "\n",
    "def toArray(item):\n",
    "  return item if hasattr(item, '__iter__') and not isinstance(item, str) else [item]\n",
    "\n",
    "def priority(priority_list):\n",
    "  return lambda items: next((type for type in priority_list if type in items.array), None)\n",
    "\n",
    "merge_schema = {\n",
    "  'FULL_NAME': 'first',\n",
    "  'NM_AUTOR': agg_to_dict,\n",
    "  'NM_ABNT_AUTOR': agg_to_dict,\n",
    "  'FIRST_LAST_NAME': agg_to_dict,\n",
    "  'TP_AUTOR': agg_to_dict,\n",
    "  'NM_TP_CATEGORIA_DOCENTE': agg_to_dict,\n",
    "  'NM_NIVEL_DISCENTE': agg_to_dict,\n",
    "  'CD_PROGRAMA_IES': agg_to_dict,\n",
    "  'NM_PROGRAMA_IES': agg_to_dict,\n",
    "  'NM_AREA_CONHECIMENTO': agg_to_dict,\n",
    "  'SG_ENTIDADE_ENSINO': agg_to_dict,\n",
    "  'ID_ADD_PRODUCAO_INTELECTUAL': list,\n",
    "}\n",
    "\n",
    "# 'NM_AUTOR': most_frequent,\n",
    "# 'NM_ABNT_AUTOR': most_frequent,\n",
    "# 'TP_AUTOR': higher_priority(['DOCENTE', 'EGRESSO', 'PÓS-DOC', 'DISCENTE', 'PARTICIPANTE EXTERNO']),\n",
    "# 'NM_TP_CATEGORIA_DOCENTE': higher_priority(['PERMANENTE', 'COLABORADOR', 'VISITANTE']),\n",
    "# 'NM_NIVEL_DISCENTE': higher_priority(['DOUTORADO PROFISSIONAL', 'BACHARELADO', 'MESTRADO', 'DOUTORADO', 'MESTRADO PROFISSIONAL', ]),\n",
    "# 'NM_PROGRAMA_IES': most_frequent,\n",
    "# 'NM_AREA_CONHECIMENTO': most_frequent,\n",
    "# 'SG_ENTIDADE_ENSINO': most_frequent,\n",
    "# 'ID_ADD_PRODUCAO_INTELECTUAL': list,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df by ID\n",
    "print('⚙️ Merging authors by ID...')\n",
    "merged_authors = df.groupby(['ID'], sort=False, as_index=False).agg(merge_schema)\n",
    "print(\"   {} authors with ID after merge\".format(len(merged_authors)))\n",
    "\n",
    "merged_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the authors without an ID (orphan authors)\n",
    "orphan_authors = df[df['ID'].isnull()]\n",
    "print(\"   {} authors without IDs\".format(len(orphan_authors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two authors and return a value `n` indicating the probability that both authors are the same person\n",
    "def compare_authors(author, orphan):\n",
    "  n = 0\n",
    "\n",
    "  # Exact name match\n",
    "  if orphan['FULL_NAME'] in author['NM_AUTOR']: n = n + 5;\n",
    "  # Match first and last name\n",
    "  if orphan['FIRST_LAST_NAME'] in author['FIRST_LAST_NAME']: n = n + 2;\n",
    "\n",
    "  # Return if there's no chance of match\n",
    "  if n == 0: return 0\n",
    "\n",
    "  # Match abnt name\n",
    "  if orphan['NM_ABNT_AUTOR'] in author['NM_ABNT_AUTOR']: n = n + 1;\n",
    "  # Match university\n",
    "  if orphan['SG_ENTIDADE_ENSINO'] in author['SG_ENTIDADE_ENSINO']: n = n + 1;\n",
    "  # Match author type\n",
    "  if orphan['TP_AUTOR'] in author['TP_AUTOR']: n = n + 1;\n",
    "  # Match IES program\n",
    "  if orphan['CD_PROGRAMA_IES'] in author['CD_PROGRAMA_IES']: n = n + 1;\n",
    "\n",
    "  return n\n",
    "\n",
    "def update_item_count(item_count, value):\n",
    "  item_count[value] = 1 if value not in item_count else item_count[value] + 1\n",
    "\n",
    "def merge_authors(author, orphan_author):\n",
    "  merged = author.copy(deep=True)\n",
    "  for column in author.index.to_list():\n",
    "    if column not in merge_schema or column == 'FULL_NAME': continue\n",
    "    author_value = author[column]\n",
    "    orphan_value = orphan_author[column]\n",
    "    if isinstance(author_value, list):\n",
    "      author_value.append(orphan_value)\n",
    "    else:\n",
    "      update_item_count(author_value, orphan_value)\n",
    "\n",
    "  return merged\n",
    "\n",
    "# Merging orphan authors\n",
    "merge_count = 0\n",
    "append_count = 0\n",
    "\n",
    "p_authors = merged_authors\n",
    "\n",
    "def process_null_author(idx_na):\n",
    "  global merged_authors, p_authors, merge_count, append_count\n",
    "  \n",
    "  try:\n",
    "    orphan = orphan_authors.iloc[idx_na]\n",
    "    last_name = orphan['FULL_NAME'].split(' ')[-1]\n",
    "    potential_authors = merged_authors[merged_authors['FULL_NAME'].str.contains(last_name, na=False)]\n",
    "    \n",
    "    for idx_pot in range(len(potential_authors)):\n",
    "      author = potential_authors.iloc[idx_pot]\n",
    "      \n",
    "      if compare_authors(author, orphan) >= 5:\n",
    "        print(\"   🔄 Merging authors ({})'{}' to ({})'{}'\".format(idx_na, orphan['FULL_NAME'], idx_pot, author['FULL_NAME']))\n",
    "        merged = merge_authors(author, orphan)\n",
    "        p_authors.loc[[author.name]] = pd.DataFrame(merged)\n",
    "        merge_count = merge_count + 1\n",
    "        return\n",
    "    print(\"   Appending author ({})'{}'\".format(idx_na, orphan['NM_AUTOR']))\n",
    "    orphan_df = pd.DataFrame(orphan).T.groupby(['NM_AUTOR']).agg(merge_schema)\n",
    "    p_authors = pd.concat([p_authors, orphan_df], ignore_index=True)\n",
    "    append_count = append_count + 1\n",
    "  except:\n",
    "    print(\"   Error processing author ({})'{}', '{}'  -- skipping\".format(idx_na, orphan['NM_AUTOR'], orphan['NM_ABNT_AUTOR']))\n",
    "    pass\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "print('⚙️ Merging authors without IDs (using {} cores):'.format(num_cores))\n",
    "\n",
    "# Single thread processing (keep commented)\n",
    "# for i in range(len(orphan_authors)): process_null_author(i)\n",
    "\n",
    "# Parallel processing\n",
    "Parallel(n_jobs=num_cores, require='sharedmem')(delayed(process_null_author)(i) for i in range(len(orphan_authors)))\n",
    "\n",
    "print(\"   {} authors were merged and {} were appended to the dataset.\".format(merge_count, append_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('⚙️ Exporting authors to processed_authors.csv...')\n",
    "\n",
    "p_authors.to_csv('processed_authors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_a = 21335 # 79797, 21335, 171\n",
    "# idx_na = 2 # 0, 2, 13\n",
    "# merged_authors.iloc[[idx_a]] = merge_authors(merged_authors.iloc[[idx_a]], orphan_authors.iloc[[idx_na]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = merged_authors.loc[[58693]]\n",
    "# b = orphan_authors.loc[[1051112]]\n",
    "\n",
    "# display(a)\n",
    "# display(b)\n",
    "\n",
    "# m = merge_authors(a, b)\n",
    "\n",
    "# display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_authors = pd.concat([merged_authors, orphan_authors], ignore_index=True)\n",
    "# merged_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_authors.to_excel('processed_authors.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_authors(author, orphan_author):\n",
    "#   merged = author.copy(deep=True)\n",
    "#   for column in author.index.to_list():\n",
    "#     print(column)\n",
    "#     if column not in merge_schema: continue\n",
    "#     agg_func = merge_schema[column]\n",
    "#     itemA = toArray(author.get(column).iloc[0])\n",
    "#     itemB = toArray(orphan_author.get(column).iloc[0])\n",
    "#     value = agg_func(pd.Series([*itemA, *itemB]))\n",
    "#     merged[column] = [value]\n",
    "#   return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_authors[merged_authors['NM_AUTOR'].str.contains(\"RECAMONDE\", na=False)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a94588eda9d64d9e9a351ab8144e55b1fabf5113b54e67dd26a8c27df0381b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
